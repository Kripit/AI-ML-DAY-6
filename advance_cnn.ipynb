{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474c4cfe",
   "metadata": {},
   "source": [
    "# ADVANCE CNN PROJECT \n",
    "\n",
    "* Before moving forward to rnn let me introduce you all to advance cnn\n",
    "\n",
    "* This is gonna be one good level code with explanation \n",
    "\n",
    "# About Project : -\n",
    "\n",
    "1. deepfake detection model , it detect fake images by real images this is the important point  , WE GONNA USE VIT TRANSFORMERS\n",
    "\n",
    "2. VIT with multi -scale Fusion ->  multi-scale attention with pretrained VIT  that focus oon artifacts of deepfake \n",
    "\n",
    "3. Multi - Scale Inference : 3 scales (224,256,192)  for robustness\n",
    "\n",
    "4. Mixed Pecision + Gradient Accumulation : for 8gb vram optimized\n",
    "\n",
    "5. Dropout + Adaptive LR : for overfitting and convergence \n",
    "\n",
    "\n",
    "# what is VIT ?\n",
    "\n",
    "* Transformer - based image model , that uses patches and self-attention \n",
    "\n",
    "\n",
    " * Example Output : - \n",
    "\n",
    "\n",
    "- 2025-08-12 19:20:00,123 - __main__ - INFO - Using device: cuda\n",
    "- 2025-08-12 19:20:05,456 - __main__ - INFO - Initialized train dataset with 3500 images\n",
    "...\n",
    "- 2025-08-12 19:30:00,123 - __main__ - INFO - Epoch 25/25, Loss: 0.0756\n",
    "- 2025-08-12 19:30:00,456 - __main__ - INFO - Validation Accuracy: 95.80%\n",
    "- 2025-08-12 19:30:00,789 - __main__ - INFO - Test Accuracy: 95.60%\n",
    "- 2025-08-12 19:30:01,012 - __main__ - INFO - Multi-Scale Prediction: 0\n",
    "Final Prediction: Real\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25d0fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.0.88)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mogge\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mogge\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mogge\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl (275 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2025.7.34 safetensors-0.6.2 tokenizers-0.21.4 transformers-4.55.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'c:\\Users\\mogge\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision opencv-python-headless transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e767a2",
   "metadata": {},
   "source": [
    "# EXPLAINING LIBRARIES: \n",
    "\n",
    "1. import torch.nn as nn -> neural network building blocks : layets(Linear , Conv3d , BatchNorm), activation , loss functions \n",
    "\n",
    "one use nn.Module to define cutom module architectures\n",
    "\n",
    "2. import torch.optim as optim -> optimizers algorithm like Adam , SGD , AdamW to update model weights\n",
    "\n",
    "Links model gradient from backward() to actual parameter updates \n",
    "\n",
    "3. from transformers import ViTModel , ViTConfig \n",
    "\n",
    "- ViTModel - > pretrained vision transformer backbone - handkes patch embedding , attention layers , position encoding \n",
    "\n",
    "- ViTConfig -> Defines architecture params (hidden size , attention heads , etc) if building from scratch or customization \n",
    "\n",
    "4. import torchvision.transformers as transformers -> Image data augmentation and normalization pipeline \n",
    "\n",
    "- Commonly chained in transforers.Composer() for resizing , cropping , normalization , tensor conversion\n",
    "\n",
    "5. from torch.utils.data import Dataset , DataLoader -> \n",
    "\n",
    "- Dataset - Custom class interface for how your data is stored and retrieved \n",
    "\n",
    "- DataLoader - handles batching , shuffling and multi-threaded loading for efficient training \n",
    "\n",
    "6. import torch.cuda.amp import autocast , GradeScaler\n",
    "\n",
    "- autocast - Automatically chooses FP16 OR FP32 operations to reduce memory and imporve training speed\n",
    "\n",
    "- GradScaler - prevents underfloe when training in mixed precision by scalling loss before backward()\n",
    "\n",
    "7. from torch.optim.lr_scheduler import OneCycleLR - adjning rate dynamically over training asteps according to the 1 cycle policy (warm-up , ramp-up , decay )\n",
    "\n",
    "- helps models coverge faster an acoid local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# logging setup\n",
    "\n",
    "logging.basicConfig(level = logging.INFO , format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                   \n",
    "                   handlers = [logging.StreamHandler(), logging.FileHandler('deepfake_detection.log')]\n",
    "                    )\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"./deepfake_dataset\"\n",
    "        self.classes = ['real','deepfake']\n",
    "        self.train_split = 0.7\n",
    "        self.val_split = 0.2\n",
    "        self.test_split = 0.1\n",
    "        self.batch_size = 2\n",
    "        self.patch_size = 16\n",
    "        self.img_size = 224\n",
    "        self.accumulation_steps = 2\n",
    "        self.epochs = 25\n",
    "        self.attention_heads = 16\n",
    "        self.model_path ='deepfake_vit_model.pt'\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lr= 0.0001\n",
    "        self.max_lr = 0.0015\n",
    "        self.weight_decay = 1e-4 \n",
    "        self.dropout_rate = 0.2\n",
    "        logger.info(f\"Using device {self.device}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838506b9",
   "metadata": {},
   "source": [
    "## Vision Transformer Parameters Explained (Simplified)\n",
    "\n",
    "- **Patch Size = 16**  \n",
    "  The image is split into small squares of `16×16` pixels.  \n",
    "  For a `224×224` image:  \n",
    "  (224 / 16) × (224 / 16) = 14 × 14 = 196 patches\n",
    "\n",
    "**196 patches** means the model sees the image as **196 small images**.\n",
    "\n",
    "- **Gradient Accumulation Steps = 2**  \n",
    "Instead of updating the model after every batch, we wait for **2 batches** before updating.  \n",
    "This makes the *effective batch size* bigger without needing more GPU memory.\n",
    "\n",
    "- **Attention Heads = 16**  \n",
    "The transformer looks at the image patches from **16 different perspectives** at the same time.  \n",
    "This helps it notice different types of patterns and relationships.\n",
    "\n",
    "- **Model Path = 'deepfake_vit_model.pt'**  \n",
    "Where the trained model is saved so we can use it later without retraining.\n",
    "\n",
    "- **Weight Decay = 1e-4**  \n",
    "Stops the model's weights from growing too big (reduces overfitting).  \n",
    "Think of it like a small “shrink” effect on weights during training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
